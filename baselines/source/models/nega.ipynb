{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "from torch.nn import TransformerEncoderLayer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "from scipy.signal import hilbert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load('../AL-NEGAT/Data/ABIDE2.npy',allow_pickle=True)\n",
    "filename = '../AL-NEGAT/Data/ABIDEII_Composite_Phenotypic.csv'\n",
    "csv2dict = pd.read_csv(filename,encoding='windows-1252').to_dict()\n",
    "ID_list = np.array(list(csv2dict['SUB_ID'].values()))\n",
    "FC_mat = []\n",
    "T1_mat = []\n",
    "lbl_arr = []\n",
    "for id in ID_list:\n",
    "    try:\n",
    "        if data[id]['FC'].shape[0]==data[id]['T1'].shape[0]:\n",
    "            FC_mat.append(data[id]['FC'])\n",
    "            T1_mat.append(data[id]['T1'])\n",
    "            lbl_arr.append(data[id]['label'])\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "FC_mat = np.array(FC_mat)\n",
    "T1_mat = np.array(T1_mat)\n",
    "lbl_arr = np.array(lbl_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((546, 400, 400), (546, 400, 4), (546,))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FC_mat.shape, T1_mat.shape, lbl_arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.46886446886446886"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lbl_arr[lbl_arr==2] = 0\n",
    "lbl_arr.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        z, x, y = self.data[idx]\n",
    "        z = torch.from_numpy(z.astype(np.float32))\n",
    "        x = torch.from_numpy(x.astype(np.float32))\n",
    "        edge_indx = torch.tensor(np.indices((400,400)).reshape(2,-1),dtype=torch.long).t().contiguous()\n",
    "        iden_mat = torch.eye(400,dtype=torch.float32)\n",
    "        y = torch.tensor(y)\n",
    "        return z,x,edge_indx,iden_mat,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#random number generator for train test split\n",
    "rng = np.random.RandomState(42)\n",
    "indices = np.arange(len(FC_mat))\n",
    "rng.shuffle(indices)\n",
    "FC_mat = FC_mat[indices]\n",
    "T1_mat = T1_mat[indices]\n",
    "lbl_arr = lbl_arr[indices]\n",
    "\n",
    "data_dict = {}\n",
    "for i in range(len(FC_mat)):\n",
    "    data_dict[i] = [FC_mat[i], T1_mat[i], lbl_arr[i]]\n",
    "\n",
    "train_data = CustomDataset(list(data_dict.values())[:int(0.8*len(data_dict))])\n",
    "test_data = CustomDataset(list(data_dict.values())[int(0.8*len(data_dict)):])\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=1, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn.conv import GCNConv\n",
    "\n",
    "class NEGA_block(torch.nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.node_gcn = GCNConv(4,4)\n",
    "        self.edge_gcn = GCNConv(400,400)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        #batchnorm with relu\n",
    "        self.batchnorm_relu = nn.Sequential(\n",
    "            nn.BatchNorm1d(400),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.W_z = nn.Parameter(torch.randn(400,400))\n",
    "        self.W_x = nn.Parameter(torch.randn(4,4))\n",
    "\n",
    "    def forward(self, z,x,edge_indx,iden_mat):\n",
    "        \n",
    "        print('Red was here')\n",
    "        v = self.edge_gcn(x = iden_mat ,edge_index=edge_indx ,edge_weight= torch.flatten(z,1,-1))#400x400\n",
    "        print('Red survived')\n",
    "        qk = self.node_gcn(x,edge_indx)#400x4\n",
    "        A = self.softmax(qk@qk.T)*v\n",
    "\n",
    "        Z_lp1 = self.batchnorm_relu(A@z@self.W_z)+z\n",
    "        X_lp1 = self.batchnorm_relu(A@x@self.W_x)+x\n",
    "        return Z_lp1, X_lp1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create 5 layers of NEGA blocks and flatten the output and apply mlp individually on node and edge features and concatenate them and apply mlp on the concatenated features for classification\n",
    "class NEGA(torch.nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.nega1 = NEGA_block()\n",
    "        self.nega2 = NEGA_block()\n",
    "        self.nega3 = NEGA_block()\n",
    "        self.nega4 = NEGA_block()\n",
    "        self.nega5 = NEGA_block()\n",
    "        self.mlp_node = nn.Sequential(\n",
    "            nn.Linear(4,2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2,1)\n",
    "        )\n",
    "        self.mlp_edge = nn.Sequential(\n",
    "            nn.Linear(400,200),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(200,1)\n",
    "        )\n",
    "        self.mlp_concat = nn.Sequential(\n",
    "            nn.Linear(2,1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1,1)\n",
    "        )\n",
    "    def forward(self, z,x,edge_indx,iden_mat):\n",
    "        z1,x1 = self.nega1(z,x,edge_indx,iden_mat)\n",
    "        z2,x2 = self.nega2(z1,x1,edge_indx,iden_mat)\n",
    "        z3,x3 = self.nega3(z2,x2,edge_indx,iden_mat)\n",
    "        z4,x4 = self.nega4(z3,x3,edge_indx,iden_mat)\n",
    "        z5,x5 = self.nega5(z4,x4,edge_indx,iden_mat)\n",
    "\n",
    "        node_out = self.mlp_node(x5)\n",
    "        edge_out = self.mlp_edge(z5)\n",
    "        concat_out = self.mlp_concat(torch.cat((node_out,edge_out),dim=1))\n",
    "        return concat_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Red was here\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 1 is out of bounds for dimension 0 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m y \u001b[39m=\u001b[39m y\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     15\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> 16\u001b[0m out \u001b[39m=\u001b[39m model(z,x,edge_indx,iden_mat)\n\u001b[1;32m     17\u001b[0m loss \u001b[39m=\u001b[39m criterion(out,y\u001b[39m.\u001b[39munsqueeze(\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mfloat())\n\u001b[1;32m     18\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/.conda/envs/temporal/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[19], line 26\u001b[0m, in \u001b[0;36mNEGA.forward\u001b[0;34m(self, z, x, edge_indx, iden_mat)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, z,x,edge_indx,iden_mat):\n\u001b[0;32m---> 26\u001b[0m     z1,x1 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnega1(z,x,edge_indx,iden_mat)\n\u001b[1;32m     27\u001b[0m     z2,x2 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnega2(z1,x1,edge_indx,iden_mat)\n\u001b[1;32m     28\u001b[0m     z3,x3 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnega3(z2,x2,edge_indx,iden_mat)\n",
      "File \u001b[0;32m~/.conda/envs/temporal/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[18], line 20\u001b[0m, in \u001b[0;36mNEGA_block.forward\u001b[0;34m(self, z, x, edge_indx, iden_mat)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, z,x,edge_indx,iden_mat):\n\u001b[1;32m     19\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mRed was here\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 20\u001b[0m     v \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49medge_gcn(x \u001b[39m=\u001b[39;49m iden_mat ,edge_index\u001b[39m=\u001b[39;49medge_indx ,edge_weight\u001b[39m=\u001b[39;49m torch\u001b[39m.\u001b[39;49mflatten(z,\u001b[39m1\u001b[39;49m,\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m))\u001b[39m#400x400\u001b[39;00m\n\u001b[1;32m     21\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mRed survived\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     22\u001b[0m     qk \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnode_gcn(x,edge_indx)\u001b[39m#400x4\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/temporal/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/temporal/lib/python3.10/site-packages/torch_geometric/nn/conv/gcn_conv.py:210\u001b[0m, in \u001b[0;36mGCNConv.forward\u001b[0;34m(self, x, edge_index, edge_weight)\u001b[0m\n\u001b[1;32m    208\u001b[0m cache \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_cached_edge_index\n\u001b[1;32m    209\u001b[0m \u001b[39mif\u001b[39;00m cache \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 210\u001b[0m     edge_index, edge_weight \u001b[39m=\u001b[39m gcn_norm(  \u001b[39m# yapf: disable\u001b[39;49;00m\n\u001b[1;32m    211\u001b[0m         edge_index, edge_weight, x\u001b[39m.\u001b[39;49msize(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnode_dim),\n\u001b[1;32m    212\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mimproved, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madd_self_loops, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mflow, x\u001b[39m.\u001b[39;49mdtype)\n\u001b[1;32m    213\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcached:\n\u001b[1;32m    214\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_cached_edge_index \u001b[39m=\u001b[39m (edge_index, edge_weight)\n",
      "File \u001b[0;32m~/.conda/envs/temporal/lib/python3.10/site-packages/torch_geometric/nn/conv/gcn_conv.py:91\u001b[0m, in \u001b[0;36mgcn_norm\u001b[0;34m(edge_index, edge_weight, num_nodes, improved, add_self_loops, flow, dtype)\u001b[0m\n\u001b[1;32m     88\u001b[0m num_nodes \u001b[39m=\u001b[39m maybe_num_nodes(edge_index, num_nodes)\n\u001b[1;32m     90\u001b[0m \u001b[39mif\u001b[39;00m add_self_loops:\n\u001b[0;32m---> 91\u001b[0m     edge_index, edge_weight \u001b[39m=\u001b[39m add_remaining_self_loops(\n\u001b[1;32m     92\u001b[0m         edge_index, edge_weight, fill_value, num_nodes)\n\u001b[1;32m     94\u001b[0m \u001b[39mif\u001b[39;00m edge_weight \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     95\u001b[0m     edge_weight \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mones((edge_index\u001b[39m.\u001b[39msize(\u001b[39m1\u001b[39m), ), dtype\u001b[39m=\u001b[39mdtype,\n\u001b[1;32m     96\u001b[0m                              device\u001b[39m=\u001b[39medge_index\u001b[39m.\u001b[39mdevice)\n",
      "File \u001b[0;32m~/.conda/envs/temporal/lib/python3.10/site-packages/torch_geometric/utils/loop.py:340\u001b[0m, in \u001b[0;36madd_remaining_self_loops\u001b[0;34m(edge_index, edge_attr, fill_value, num_nodes)\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Adds remaining self-loop :math:`(i,i) \\in \\mathcal{E}` to every node\u001b[39;00m\n\u001b[1;32m    307\u001b[0m \u001b[39m:math:`i \\in \\mathcal{V}` in the graph given by :attr:`edge_index`.\u001b[39;00m\n\u001b[1;32m    308\u001b[0m \u001b[39mIn case the graph is weighted or has multi-dimensional edge features\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[39m    tensor([0.5000, 0.5000, 1.0000, 1.0000]))\u001b[39;00m\n\u001b[1;32m    338\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    339\u001b[0m N \u001b[39m=\u001b[39m maybe_num_nodes(edge_index, num_nodes)\n\u001b[0;32m--> 340\u001b[0m mask \u001b[39m=\u001b[39m edge_index[\u001b[39m0\u001b[39m] \u001b[39m!=\u001b[39m edge_index[\u001b[39m1\u001b[39;49m]\n\u001b[1;32m    342\u001b[0m loop_index \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39marange(\u001b[39m0\u001b[39m, N, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mlong, device\u001b[39m=\u001b[39medge_index\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m    343\u001b[0m loop_index \u001b[39m=\u001b[39m loop_index\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mrepeat(\u001b[39m2\u001b[39m, \u001b[39m1\u001b[39m)\n",
      "\u001b[0;31mIndexError\u001b[0m: index 1 is out of bounds for dimension 0 with size 1"
     ]
    }
   ],
   "source": [
    "#train\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = NEGA().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', patience=5, factor=0.5, verbose=True)\n",
    "model.train()\n",
    "for epoch in range(100):\n",
    "    for i, (z,x,edge_indx,iden_mat,y) in enumerate(train_loader):\n",
    "        z = z.to(device)\n",
    "        x = x.to(device)\n",
    "        edge_indx = edge_indx.to(device)\n",
    "        iden_mat = iden_mat.to(device)\n",
    "        y = y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(z,x,edge_indx,iden_mat)\n",
    "        loss = criterion(out,y.unsqueeze(1).float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print('Epoch: ',epoch,' Loss: ',loss.item())\n",
    "    scheduler.step(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "temporal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
