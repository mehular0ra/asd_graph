{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "from torch.nn import TransformerEncoderLayer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "from scipy.signal import hilbert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load('../AL-NEGAT/Data/ABIDE2.npy',allow_pickle=True)\n",
    "filename = '../AL-NEGAT/Data/ABIDEII_Composite_Phenotypic.csv'\n",
    "csv2dict = pd.read_csv(filename,encoding='windows-1252').to_dict()\n",
    "ID_list = np.array(list(csv2dict['SUB_ID'].values()))\n",
    "FC_mat = []\n",
    "T1_mat = []\n",
    "lbl_arr = []\n",
    "for id in ID_list:\n",
    "    try:\n",
    "        if data[id]['FC'].shape[0]==data[id]['T1'].shape[0]:\n",
    "            FC_mat.append(data[id]['FC'])\n",
    "            T1_mat.append(data[id]['T1'])\n",
    "            lbl_arr.append(data[id]['label'])\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "FC_mat = np.array(FC_mat)\n",
    "T1_mat = np.array(T1_mat)\n",
    "lbl_arr = np.array(lbl_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((546, 400, 400), (546, 400, 4), (546,))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FC_mat.shape, T1_mat.shape, lbl_arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.46886446886446886"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lbl_arr[lbl_arr==2] = 0\n",
    "lbl_arr.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        z, x, y = self.data[idx]\n",
    "        z = torch.from_numpy(z.astype(np.float32))\n",
    "        x = torch.from_numpy(x.astype(np.float32))\n",
    "        # edge_indx = torch.tensor(np.indices((400,400)).reshape(2,-1),dtype=torch.long).t().contiguous()\n",
    "        # iden_mat = torch.eye(400,dtype=torch.float32)\n",
    "        y = torch.tensor(y)\n",
    "        return z,x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#random number generator for train test split\n",
    "rng = np.random.RandomState(42)\n",
    "indices = np.arange(len(FC_mat))\n",
    "rng.shuffle(indices)\n",
    "FC_mat = FC_mat[indices]\n",
    "T1_mat = T1_mat[indices]\n",
    "lbl_arr = lbl_arr[indices]\n",
    "\n",
    "data_dict = {}\n",
    "for i in range(len(FC_mat)):\n",
    "    data_dict[i] = [FC_mat[i], T1_mat[i], lbl_arr[i]]\n",
    "\n",
    "train_data = CustomDataset(list(data_dict.values())[:int(0.8*len(data_dict))])\n",
    "test_data = CustomDataset(list(data_dict.values())[int(0.8*len(data_dict)):])\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NEGA_block(torch.nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.Wqk = nn.Parameter(torch.randn(400,4))\n",
    "        self.Wv = nn.Parameter(torch.randn(400,400))\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        #batchnorm with relu\n",
    "        self.batchnorm_relu = nn.Sequential(\n",
    "            nn.BatchNorm1d(400),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.W_z = nn.Parameter(torch.randn(400,400))\n",
    "        self.W_x = nn.Parameter(torch.randn(4,4))\n",
    "\n",
    "    def forward(self, z,x):\n",
    "        \n",
    "        v = z*self.Wv\n",
    "        qk = x*self.Wqk\n",
    "        # print(qk.shape, v.shape)\n",
    "        A = self.softmax(qk@torch.transpose(qk,1,2))*v\n",
    "\n",
    "        Z_lp1 = self.batchnorm_relu(A@z@self.W_z)+z\n",
    "        X_lp1 = self.batchnorm_relu(A@x@self.W_x)+x\n",
    "        return Z_lp1, X_lp1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create 5 layers of NEGA blocks and flatten the output and apply mlp individually on node and edge features and concatenate them and apply mlp on the concatenated features for classification\n",
    "class NEGA(torch.nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.nega1 = NEGA_block()\n",
    "        self.nega2 = NEGA_block()\n",
    "        self.nega3 = NEGA_block()\n",
    "        self.nega4 = NEGA_block()\n",
    "        self.nega5 = NEGA_block()\n",
    "        self.mlp_node = nn.Sequential(\n",
    "            nn.Linear(4*400,128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128,16),\n",
    "            nn.Dropout(0.5)\n",
    "        )\n",
    "        self.mlp_edge = nn.Sequential(\n",
    "            nn.Linear(160000,512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512,16),\n",
    "            nn.Dropout(0.5)\n",
    "        )\n",
    "        self.mlp_concat = nn.Sequential(\n",
    "            nn.Linear(32,16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16,1)\n",
    "        )\n",
    "    def forward(self, z,x):\n",
    "        z1,x1 = self.nega1(z,x)\n",
    "        z2,x2 = self.nega2(z1,x1)\n",
    "        z3,x3 = self.nega3(z2,x2)\n",
    "        z4,x4 = self.nega4(z3,x3)\n",
    "        z5,x5 = self.nega5(z4,x4)\n",
    "        # print(x5.flatten(1,2).shape, z5.flatten(1,2).shape)\n",
    "\n",
    "        node_out = self.mlp_node(x5.flatten(1,2))\n",
    "        edge_out = self.mlp_edge(z5.flatten(1,2))\n",
    "        concat_out = self.mlp_concat(torch.cat((node_out,edge_out),dim=1))\n",
    "\n",
    "        return concat_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Train Loss: 35.74465560913086 Test Loss: 8.053668975830078\n",
      "Epoch: 1 Train Loss: 2.3253366947174072 Test Loss: 0.9752073287963867\n",
      "Epoch: 2 Train Loss: 1.3835289478302002 Test Loss: 2.8403632640838623\n",
      "Epoch: 3 Train Loss: 1.217729926109314 Test Loss: 1.4212478399276733\n",
      "Epoch: 4 Train Loss: 1.3130813837051392 Test Loss: 1.469170331954956\n",
      "Epoch: 5 Train Loss: 0.376838743686676 Test Loss: 1.9218261241912842\n",
      "Epoch: 6 Train Loss: 0.06437404453754425 Test Loss: 2.784975290298462\n",
      "Epoch: 7 Train Loss: 0.00044554146006703377 Test Loss: 3.0164759159088135\n",
      "Epoch: 8 Train Loss: 0.0021474198438227177 Test Loss: 0.9099162220954895\n",
      "Epoch: 9 Train Loss: 0.0009143541101366282 Test Loss: 1.444753885269165\n",
      "Epoch: 10 Train Loss: 0.0008626551134511828 Test Loss: 0.9214375615119934\n",
      "Epoch: 11 Train Loss: 0.0009268997237086296 Test Loss: 1.401872992515564\n",
      "Epoch: 12 Train Loss: 0.004754604771733284 Test Loss: 161.9132843017578\n",
      "Epoch 00014: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch: 13 Train Loss: 0.002088115783408284 Test Loss: 1.5014684200286865\n",
      "Epoch: 14 Train Loss: 0.006029676645994186 Test Loss: 1.2724298238754272\n",
      "Epoch: 15 Train Loss: 0.00099514564499259 Test Loss: 0.5955103635787964\n",
      "Epoch: 16 Train Loss: 0.0008916356600821018 Test Loss: 171.65325927734375\n",
      "Epoch: 17 Train Loss: 0.003076721215620637 Test Loss: 173.70126342773438\n",
      "Epoch: 18 Train Loss: 0.003237722907215357 Test Loss: 175.9525604248047\n",
      "Epoch 00020: reducing learning rate of group 0 to 2.5000e-04.\n",
      "Epoch: 19 Train Loss: 0.000559925683774054 Test Loss: 0.30702754855155945\n"
     ]
    }
   ],
   "source": [
    "#train\n",
    "import wandb\n",
    "torch.random.manual_seed(42)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = NEGA().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', patience=3, factor=0.5, verbose=True)\n",
    "model.train()\n",
    "# wandb.init(project='NEGA')\n",
    "for epoch in range(20):\n",
    "    for i, (z,x,y) in enumerate(train_loader):\n",
    "        z = z.to(device)\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(z,x)\n",
    "        train_loss = criterion(out,y.unsqueeze(1).float())\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "    scheduler.step(train_loss.item())\n",
    "    # wandb.log({'train_loss':train_loss.item()})\n",
    "\n",
    "    #test\n",
    "    model.eval()\n",
    "    for i, (z,x,y) in enumerate(test_loader):\n",
    "        z = z.to(device)\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        out = model(z,x)\n",
    "        test_loss = criterion(out,y.unsqueeze(1).float())\n",
    "    # wandb.log({'test_loss':test_loss.item()})\n",
    "    print('Epoch: {} Train Loss: {} Test Loss: {}'.format(epoch, train_loss.item(), test_loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "temporal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
