{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch.autograd import grad\n",
    "\n",
    "\n",
    "class Grad_CAM():\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.activations = None\n",
    "        self.gradient = None\n",
    "\n",
    "        # register hooks to capture the feature_map gradients\n",
    "        def forward_hook(model, input, output):\n",
    "            self.activations = output[0]\n",
    "\n",
    "        def backward_hook(model, grad_input, grad_output):\n",
    "            self.gradient = grad_output[0][0]\n",
    "\n",
    "        feat_map = model.features[29]  # Relu layer\n",
    "        feat_map.register_forward_hook(forward_hook)\n",
    "        feat_map.register_backward_hook(backward_hook)\n",
    "\n",
    "    def get_grad_cam(self, img, indices=None):\n",
    "        self.model.eval()\n",
    "        out = self.model(img)  # 1*1000\n",
    "        num_features = self.activations.size()[0]  # 512 *14*14\n",
    "        topk = 3\n",
    "        if indices == None:\n",
    "            values, indices = torch.topk(out, topk)\n",
    "        else:\n",
    "            values = torch.tensor([np.array(range(4, 1, -1))])\n",
    "            indices = torch.tensor([indices])\n",
    "        # Compute 14x14 heatmaps\n",
    "        heatmaps = torch.zeros(topk, 14, 14)\n",
    "        for i, c in enumerate(indices[0]):\n",
    "            self.model.zero_grad()\n",
    "            out[0, c].backward(retain_graph=True)  # 512 *14*14\n",
    "            # feature importance\n",
    "            feature_importance = self.gradient.mean(dim=[1, 2])  # 512\n",
    "            # pixel importance\n",
    "            for f in range(num_features):\n",
    "                heatmaps[i] += feature_importance[f] * \\\n",
    "                    self.activations[f]  # int * [14*14]    512*14*14\n",
    "            heatmaps[i] = F.relu(heatmaps[i])\n",
    "            heatmaps[i] /= torch.max(heatmaps[i])\n",
    "#             print(heatmaps[i].shape,feature_importance.shape , self.activations.shape, self.gradient.shape)\n",
    "        # Upsample to 224x224\n",
    "        large_heatmaps = F.interpolate(heatmaps.expand(\n",
    "            (1, topk, 14, 14)), (224, 224), mode='bilinear')\n",
    "        return large_heatmaps[0].data.numpy(), values.data.numpy()[0], indices.data.numpy()[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool\n",
    "from torch_geometric.data import Batch\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GCN, self).__init__()\n",
    "        self.num_layers = 2\n",
    "        self.dropout = 0.5\n",
    "        self.hidden_size = 64\n",
    "        self.num_classes = 1\n",
    "        self.node_sz = 400\n",
    "\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        for i in range(self.num_layers):\n",
    "            if i == 0:\n",
    "                self.convs.append(\n",
    "                    GCNConv(400, self.hidden_size))\n",
    "            else:\n",
    "                self.convs.append(GCNConv(self.hidden_size, self.hidden_size))\n",
    "\n",
    "        self.readout_lin = nn.Linear(\n",
    "            self.node_sz * self.hidden_size, self.hidden_size)\n",
    "\n",
    "        self.lin = nn.Linear(self.hidden_size, 1)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def convert_edge_positive(self, edge_index, edge_weight):\n",
    "        edge_index = edge_index[:, edge_weight > 0]\n",
    "        edge_weight = edge_weight[edge_weight > 0]\n",
    "        return edge_index, edge_weight\n",
    "\n",
    "    def forward(self, data, **kwargs):\n",
    "        # self.epoch = kwargs['epoch']\n",
    "        # self.iteration = kwargs['iteration']\n",
    "        # self.test_phase = kwargs['test_phase']\n",
    "\n",
    "        x, edge_index, edge_weight, batch, labels = data.x, data.edge_index, data.edge_weight, data.batch, data.y\n",
    "        edge_index, edge_weight = self.convert_edge_positive(\n",
    "            edge_index, edge_weight)\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.convs[i](x, edge_index, edge_weight)\n",
    "            if i < self.num_layers - 1:\n",
    "                x = F.leaky_relu(x)\n",
    "\n",
    "        xs = []\n",
    "        for graph_idx in batch.unique():\n",
    "            graph_nodes = x[batch == graph_idx]\n",
    "            graph_nodes = graph_nodes.view(-1)\n",
    "            xs.append(self.readout_lin(graph_nodes))\n",
    "        x = torch.stack(xs).to(x.device)\n",
    "\n",
    "        x = F.leaky_relu(x)\n",
    "\n",
    "        x = self.lin(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attn_Net_Gated(nn.Module):\n",
    "    # Attention Network with Sigmoid Gating (3 fc layers). Args:\n",
    "    # L: input feature dimension\n",
    "    # D: hidden layer dimension\n",
    "    # dropout: whether to use dropout (p = 0.25)\n",
    "    # n_classes: number of classes \"\"\"\n",
    "\n",
    "    def __init__(self, L=64, D=256, dropout=True, n_classes=1):\n",
    "        super(Attn_Net_Gated, self).__init__()\n",
    "        self.attention_a = [nn.Linear(L, D), nn.Tanh()]\n",
    "        self.attention_b = [nn.Linear(L, D), nn.Sigmoid()]\n",
    "        if dropout:\n",
    "            self.attention_a.append(nn.Dropout(0.25))\n",
    "            self.attention_b.append(nn.Dropout(0.25))\n",
    "\n",
    "        self.attention_a = nn.Sequential(*self.attention_a)\n",
    "        self.attention_b = nn.Sequential(*self.attention_b)\n",
    "        self.attention_c = nn.Linear(D, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        a = self.attention_a(x)\n",
    "        b = self.attention_b(x)\n",
    "        A = a.mul(b)\n",
    "        A = self.attention_c(A)  # N x n_classes\n",
    "        # A = F.softmax(A, dim=0)\n",
    "        return A, x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "# from torch_geometric.nn import global_mean_pool, HypergraphConv\n",
    "from torch_geometric.data import Batch\n",
    "\n",
    "from omegaconf import DictConfig\n",
    "\n",
    "\n",
    "import ipdb\n",
    "\n",
    "\n",
    "class Attn_Net_Gated(nn.Module):\n",
    "    # Attention Network with Sigmoid Gating (3 fc layers). Args:\n",
    "    # L: input feature dimension\n",
    "    # D: hidden layer dimension\n",
    "    # dropout: whether to use dropout (p = 0.25)\n",
    "    # n_classes: number of classes \"\"\"\n",
    "\n",
    "    def __init__(self, L=64, D=256, dropout=True, n_classes=1):\n",
    "        super(Attn_Net_Gated, self).__init__()\n",
    "        self.attention_a = [nn.Linear(L, D), nn.Tanh()]\n",
    "        self.attention_b = [nn.Linear(L, D), nn.Sigmoid()]\n",
    "        if dropout:\n",
    "            self.attention_a.append(nn.Dropout(0.25))\n",
    "            self.attention_b.append(nn.Dropout(0.25))\n",
    "\n",
    "        self.attention_a = nn.Sequential(*self.attention_a)\n",
    "        self.attention_b = nn.Sequential(*self.attention_b)\n",
    "        self.attention_c = nn.Linear(D, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        a = self.attention_a(x)\n",
    "        b = self.attention_b(x)\n",
    "        A = a.mul(b)\n",
    "        A = self.attention_c(A)  # N x n_classes\n",
    "        # A = F.softmax(A, dim=0)\n",
    "        return A, x\n",
    "\n",
    "\n",
    "class DwHGN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DwHGN, self).__init__()\n",
    "\n",
    "        self.num_layers = 1\n",
    "        self.dropout = 0.5\n",
    "        self.hidden_size = 64\n",
    "        self.num_classes = 2\n",
    "        self.node_sz = 400\n",
    "\n",
    "        self.num_edges = 400\n",
    "\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        for i in range(self.num_layers):\n",
    "            if i == 0:\n",
    "                self.convs.append(DwHGNConv(\n",
    "                    self.node_sz, self.hidden_size, num_edges=self.num_edges))\n",
    "            else:\n",
    "                self.convs.append(DwHGNConv(\n",
    "                    self.node_sz, self.hidden_size, self.hidden_size, num_edges=self.num_edges))\n",
    "\n",
    "        # if self.cfg.model.readout == 'set_transformer':\n",
    "        #     self.readout_layer = SetTransformer(dim_input=self.hidden_size,\n",
    "        #                                         num_outputs=1, dim_output=self.hidden_size)\n",
    "        # elif self.cfg.model.readout == 'janossy':\n",
    "        #     self.readout_layer = JanossyPooling(\n",
    "        #         num_perm=cfg.model.num_perm, in_features=self.hidden_size, fc_out_features=self.hidden_size)\n",
    "        self.readout_lin = nn.Linear(\n",
    "            self.node_sz * self.hidden_size, self.hidden_size)\n",
    "\n",
    "        self.lin = nn.Linear(self.hidden_size, 1)\n",
    "\n",
    "        # interpretability\n",
    "        # if self.cfg.model.node_attn_interpret:\n",
    "        self.attn_gated = Attn_Net_Gated(L=self.hidden_size)\n",
    "\n",
    "    def forward(self, data, **kwargs):\n",
    "        self.epoch = kwargs['epoch']\n",
    "        self.iteration = kwargs['iteration']\n",
    "        self.test_phase = kwargs['test_phase']\n",
    "        x, hyperedge_index, hyperedge_weight, batch, labels = data.x, data.edge_index, data.edge_weight, data.batch, data.y\n",
    "        for i in range(self.num_layers):\n",
    "            # x = self.convs[i](x, hyperedge_index, hyperedge_weight, self.num_edges)\n",
    "            x = self.convs[i](x, hyperedge_index, epoch=self.epoch)\n",
    "\n",
    "            if i < self.num_layers:\n",
    "                x = F.leaky_relu(x)\n",
    "\n",
    "        # node_attn (interpretability)\n",
    "        if self.cfg.model.node_attn_interpret:\n",
    "            xs = []\n",
    "            saved_A = []\n",
    "            for graph_idx in batch.unique():\n",
    "                graph_nodes = x[batch == graph_idx]\n",
    "                A, x_new = self.attn_gated(graph_nodes)\n",
    "                saved_A.append(A.view(-1))\n",
    "                # Broadcasting A to the same dimensions as x\n",
    "                A_broadcasted = A.expand_as(x_new)\n",
    "                # Performing element-wise multiplication\n",
    "                if self.cfg.model.node_attn_learn:\n",
    "                    x_new = A_broadcasted * x_new\n",
    "                xs.append(x_new)\n",
    "            x = torch.stack(xs).to(x.device)\n",
    "            x = x.view(-1, self.hidden_size)\n",
    "\n",
    "            if self.cfg.model.node_attn_save:\n",
    "                saved_A = torch.stack(saved_A)\n",
    "                node_att_data_save(saved_A, self.epoch,\n",
    "                                   self.iteration, labels, train=True)\n",
    "\n",
    "        # if self.cfg.model.readout in ['set_transformer', 'janossy']:\n",
    "        #     x = x.view(-1, self.node_sz, self.hidden_size)\n",
    "        #     x = self.readout_layer(x)\n",
    "        #     x = x.squeeze()\n",
    "        # else:\n",
    "        xs = []\n",
    "        for graph_idx in batch.unique():\n",
    "            graph_nodes = x[batch == graph_idx]\n",
    "            graph_nodes = graph_nodes.view(-1)\n",
    "            xs.append(self.readout_lin(graph_nodes))\n",
    "        x = torch.stack(xs).to(x.device)\n",
    "        # if kwargs['test_phase'] and self.cfg.model.tsne:\n",
    "        #     tsne_plot_data(x, labels, self.epoch, self.iteration)\n",
    "\n",
    "        # if self.cfg.model.tsne:\n",
    "        #     if kwargs['test_phase']:\n",
    "        #         tsne_plot_data(x, labels, self.epoch, self.iteration)\n",
    "        #     elif self.cfg.model.tsne_train:\n",
    "        #         tsne_plot_data(x, labels, self.epoch,\n",
    "        #                        self.iteration, train=True)\n",
    "\n",
    "        x = F.leaky_relu(x)\n",
    "        x = self.lin(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelx = GCN()\n",
    "modelx.load_state_dict(torch.load(\n",
    "    '/home/mehul/asd_graph/baselines/outputs/2023-09-12/18-46-50/best_model.pt'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GCN(\n",
       "  (convs): ModuleList(\n",
       "    (0): GCNConv(400, 64)\n",
       "    (1): GCNConv(64, 64)\n",
       "  )\n",
       "  (readout_lin): Linear(in_features=25600, out_features=64, bias=True)\n",
       "  (lin): Linear(in_features=64, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '/home/mehul/asd_graph/baselines/outputs/2023-09-12/18-59-42/best_model.pt'\n",
    "import torch\n",
    "saved_weights_path = '/home/mehul/asd_graph/baselines/outputs/2023-09-12/19-07-24/best_model.pt'\n",
    "saved_state_dict = torch.load(saved_weights_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['convs.0.learned_he_weights', 'convs.0.bias', 'convs.0.lin.weight', 'readout_lin.weight', 'readout_lin.bias', 'lin.weight', 'lin.bias', 'attn_gated.attention_a.0.weight', 'attn_gated.attention_a.0.bias', 'attn_gated.attention_b.0.weight', 'attn_gated.attention_b.0.bias', 'attn_gated.attention_c.weight', 'attn_gated.attention_c.bias'])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saved_state_dict.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convs.0.learned_he_weights : torch.Size([400])\n",
      "convs.0.bias : torch.Size([64])\n",
      "convs.0.lin.weight : torch.Size([64, 400])\n",
      "readout_lin.weight : torch.Size([64, 25600])\n",
      "readout_lin.bias : torch.Size([64])\n",
      "lin.weight : torch.Size([1, 64])\n",
      "lin.bias : torch.Size([1])\n",
      "attn_gated.attention_a.0.weight : torch.Size([256, 64])\n",
      "attn_gated.attention_a.0.bias : torch.Size([256])\n",
      "attn_gated.attention_b.0.weight : torch.Size([256, 64])\n",
      "attn_gated.attention_b.0.bias : torch.Size([256])\n",
      "attn_gated.attention_c.weight : torch.Size([1, 256])\n",
      "attn_gated.attention_c.bias : torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "for key, val in saved_state_dict.items():\n",
    "    print(key, \":\", val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0583, -0.0433,  0.0879,  ...,  0.0387, -0.0656,  0.0341],\n",
       "        [ 0.0040,  0.0504,  0.0245,  ..., -0.0366, -0.0294,  0.1019],\n",
       "        [-0.0304,  0.0243, -0.0381,  ..., -0.1051,  0.0069,  0.0738],\n",
       "        ...,\n",
       "        [-0.0790, -0.0708, -0.0131,  ...,  0.1116, -0.0976, -0.0517],\n",
       "        [ 0.0122, -0.0529, -0.0747,  ..., -0.0019,  0.0374, -0.0132],\n",
       "        [-0.0894, -0.0129,  0.0185,  ..., -0.0135,  0.0117,  0.1099]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saved_state_dict['convs.0.lin.weight']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
