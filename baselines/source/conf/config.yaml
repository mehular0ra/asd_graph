defaults: 
  - dataset: fc_large
  - model: hypergraphgcn   # dwhgn,hypergraphgcn dwattnhgn,attnhgn
  - optimizer: adam
  - training: train


dataset:
  node: fc
  train_set: 0.9
  test_set: 0.1
  batch_size: 16

training:
  epochs: 100


model: 
  K_neigs: 40 #5, 25, 40, 60, 100
  num_layers: 1 #2, 3, 5
  hidden_size: 64  # 8, 16, 32, 64, 128
  dropout: 0.5     # 0.25
  heads: 1
  readout: linear #linear, set_transformer, janossy, max, mean
  attention_mode: edge
  num_perm: 25

  node_attn_interpret: True
  node_attn_learn: True
  node_attn_save: False

  tsne: False
  tsne_train: False

  gradcam: False
  ####
  model_save: False
  
  save_interpret: False
  save_epochs: [0, 19, 50, 99]

log_path: result

repeat_time: 1

leave_one_site_out: True
test_site: U_MIA
kfold: False
kfold_val: -1  # if -1: run 10fold in parallel, else: run kfold_val fold
joblib: True

# total_steps: 100 # remove this later

device: cuda:1 #cuda, cpu

# wandb:
is_wandb: True
# doing_sweep: True
project: ijcnn
entity: mehular0ra
# sweep_id: asd-graph/asd-graph/r49h4avh



hydra:
  job_logging:
    root:
      handlers: [console, file]
      propagate: true
    formatters:
      precise:
        format: '[%(asctime)s][%(filename)s][L%(lineno)d][%(levelname)s]%(message)s'
        datefmt: '%Y-%m-%d %H:%M:%S'
    handlers:
      console:
        class: logging.StreamHandler
        formatter: precise
        stream: ext://sys.stdout
      file:
        class: logging.FileHandler
        formatter: precise
        filename: ./output.log

